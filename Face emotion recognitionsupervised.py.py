# -*- coding: utf-8 -*-
"""supervised

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tfsoBhaZ74BkEE42avZ_q-W3vy5hLPZs

# ***Supervised model***

# ***Importing necessary libraries***
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from sklearn.metrics import precision_score, recall_score, f1_score
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import seaborn as sns
from sklearn.metrics import confusion_matrix
from tensorflow.keras.models import load_model
from google.colab import files
import cv2
from google.colab import output
from tensorflow.keras.models import load_model
import base64
# %matplotlib inline

"""# ***Preparing and preprocessing the FER-2013 dataset***"""

#Loading the dataest
data_path = "fer2013.csv"
df = pd.read_csv(data_path)

# Converting pixel data into images
def preprocess_pixels(pixels_str):
    pixels = np.array([float(num) for num in pixels_str.split()])
    return pixels.reshape(48, 48, 1)

df['image'] = df['pixels'].apply(preprocess_pixels)

# Normalizing images
X = np.stack(df['image'].values, axis=0)
X = X / 255.0

# One-hot encode labels
y = to_categorical(df['emotion'].values, num_classes=7)

# Splitting data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""# ***Building a CNN model for emotion recognition***"""

# CNN model
model = Sequential()

# Block 1
model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(48,48,1)))
model.add(Conv2D(32, (3,3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Block 2
model.add(Conv2D(64, (3,3), activation='relu', padding='same'))
model.add(Conv2D(64, (3,3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

# Block 3
model.add(Conv2D(128, (3,3), activation='relu', padding='same'))
model.add(Conv2D(128, (3,3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3))

# Fully Connected Layers
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(7, activation='softmax'))

"""# ***Training the CNN model***"""

# Compiling the model
model.compile(optimizer=RMSprop(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Training the model
history = model.fit(X_train, y_train,
                    validation_data=(X_test, y_test),
                    epochs=25,
                    batch_size=64)

"""# ***Saving the model***"""

# Save the trained model to an .h5 file
model.save("fer_supervised_model.h5")
print("Model saved as fer_supervised_model.h5")

from tensorflow.keras.models import load_model

# Load the saved model
model = load_model("fer_supervised_model.h5")
print("Model loaded successfully!")

"""# ***Evaluating the trained model***"""

# Evaluating the model
def secret_evaluation(model, X_test, y_test):
    results = model.evaluate(X_test, y_test, verbose=0)
    test_accuracy = results[1]  # index 0 is loss, index 1 is accuracy
    print("Exclusive Test Accuracy: {:.2f}%".format(test_accuracy * 100))

# Running the model
secret_evaluation(model, X_test, y_test)

"""# ***Getting True and Predicted Labels from the Model***"""

# Getting true and predicted labels
def get_results(model, X_input, y_input):
    predictions = model.predict(X_input)
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = np.argmax(y_input, axis=1)
    return true_labels, predicted_labels

true_labels, predicted_labels = get_results(model, X_test, y_test)

"""# ***Generating classification report***"""

# Getting models performance
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print("Classification Report:")
print(classification_report(y_true, y_pred_classes))

"""# ***Evaluating the model performance***"""

# Convert one-hot encoded labels to integers
true_labels = np.argmax(y_test, axis=1)
predicted_labels = np.argmax(model.predict(X_test), axis=1)

# Compute the weighted metrics
precision = precision_score(true_labels, predicted_labels, average='weighted')
recall = recall_score(true_labels, predicted_labels, average='weighted')
f1 = f1_score(true_labels, predicted_labels, average='weighted')

# Printing the metrics
print("Weighted Precision: {:.4f}".format(precision))
print("Weighted Recall: {:.4f}".format(recall))
print("Weighted F1 Score: {:.4f}".format(f1))

"""# ***Confusion matrix***"""

def plot_confusion_matrix_sup(y_true, y_pred):
    # Defining the emotion labels
    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
    cm = confusion_matrix(y_true, y_pred)

    # Create a DataFrame for better visualization with emotion labels
    cm_df = pd.DataFrame(cm, index=emotion_labels, columns=emotion_labels)

    # Plotting the confusion matrix
    plt.figure(figsize=(8,6))
    sns.heatmap(cm_df, annot=True, fmt="d", cmap="viridis")
    plt.title("Supervised Confusion Matrix")
    plt.xlabel("Predicted Emotion")
    plt.ylabel("True Emotion")
    plt.show()

# Call the function with the true and predicted labels
plot_confusion_matrix_sup(np.argmax(y_test, axis=1), np.argmax(model.predict(X_test), axis=1))

"""# ***Displaying Facial Emotion Images from FER-2013 Dataset***"""

# Loading the FER-2013 dataset
data_path = "/content/fer2013.csv"
df = pd.read_csv(data_path)

# Preprocessing the images
def preprocess_pixels(pixels_str):
    pixels = np.array([float(num) for num in pixels_str.split()])
    return pixels.reshape(48, 48, 1)

df['image'] = df['pixels'].apply(preprocess_pixels)

# Loading my pre-trained model
model = load_model('/content/fer_supervised_model.h5')
print("Model loaded successfully")

# Defining the emotions
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

n_samples = 15
sampled_indices = np.random.choice(df.index, n_samples, replace=False)
X_samples = np.stack(df.loc[sampled_indices, 'image'].values, axis=0)

# Normalizing the images
X_samples_normalized = X_samples / 255.0
X_samples_input = X_samples_normalized

# Predicting the emotions
predictions = model.predict(X_samples_input)
predicted_labels = [emotion_labels[np.argmax(pred)] for pred in predictions]
fig, axes = plt.subplots(3, 5, figsize=(15, 9))
axes = axes.ravel()
for i in range(n_samples):
    axes[i].imshow(X_samples[i].reshape(48, 48), cmap='gray')
    axes[i].set_title(f"Predicted: {predicted_labels[i]}")
    axes[i].axis('off')
plt.tight_layout()
plt.show()

"""# ***Testing the model***"""

# uploading the image file
uploaded = files.upload()

# Taking the name from the image file
image_path = next(iter(uploaded.keys()))
print("Uploaded file:", image_path)

# Loading my trained model
model_path = "/content/fer_supervised_model.h5"
model = load_model(model_path)
print("Model loaded successfully.")

# Classes
class_labels = {
    0: 'Angry',
    1: 'Disgust',
    2: 'Fear',
    3: 'Happy',
    4: 'Sad',
    5: 'Surprise',
    6: 'Neutral'
}

# Face detection
def detect_and_predict_emotion(image_path, model):
    # Load Haar Cascade for face detection
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Reading the uploaded image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Error: Could not load {image_path}")
        return

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    if len(faces) == 0:
        print("No face detected in the uploaded image.")
        show_image_with_label(img, "No Face Detected")
        return
    else:
        x, y, w, h = faces[0]
        face_roi = gray[y:y+h, x:x+w]
        face_resized = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_LINEAR)
        face_resized = face_resized.reshape(1, 48, 48, 1).astype('float32') / 255.0

        # Predicting the emotion
        preds = model.predict(face_resized)
        pred_class = np.argmax(preds[0])
        emotion_label = class_labels.get(pred_class, "Unknown")

        # Drawing the box around the face
        cv2.rectangle(img, (x, y), (x+w, y+h), (0,255,0), 2)
        cv2.putText(img, emotion_label, (x, y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)

        # Displaying the final image
        show_image_with_label(img, f"Predicted Emotion: {emotion_label}")

def show_image_with_label(bgr_img, title):
    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(8, 8))
    plt.imshow(rgb_img)
    plt.title(title)
    plt.axis("off")
    plt.show()
detect_and_predict_emotion(image_path, model)

# uploading the image file
uploaded = files.upload()

# Taking the name from the image file
image_path = next(iter(uploaded.keys()))
print("Uploaded file:", image_path)

# Loading my trained model
model_path = "/content/fer_supervised_model.h5"
model = load_model(model_path)
print("Model loaded successfully.")

# Classes
class_labels = {
    0: 'Angry',
    1: 'Disgust',
    2: 'Fear',
    3: 'Happy',
    4: 'Sad',
    5: 'Surprise',
    6: 'Neutral'
}

# Face detection
def detect_and_predict_emotion(image_path, model):
    # Load Haar Cascade for face detection
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Reading the uploaded image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Error: Could not load {image_path}")
        return

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    if len(faces) == 0:
        print("No face detected in the uploaded image.")
        show_image_with_label(img, "No Face Detected")
        return
    else:
        x, y, w, h = faces[0]
        face_roi = gray[y:y+h, x:x+w]
        face_resized = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_LINEAR)
        face_resized = face_resized.reshape(1, 48, 48, 1).astype('float32') / 255.0

        # Predicting the emotion
        preds = model.predict(face_resized)
        pred_class = np.argmax(preds[0])
        emotion_label = class_labels.get(pred_class, "Unknown")

        # Draw the box around the face
        cv2.rectangle(img, (x, y), (x+w, y+h), (0,255,0), 2)
        cv2.putText(img, emotion_label, (x, y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)

        # Displaying the final image
        show_image_with_label(img, f"Predicted Emotion: {emotion_label}")

def show_image_with_label(bgr_img, title):
    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(8, 8))
    plt.imshow(rgb_img)
    plt.title(title)
    plt.axis("off")
    plt.show()
detect_and_predict_emotion(image_path, model)

# uploading the image file
uploaded = files.upload()

# Taking the name from the image file
image_path = next(iter(uploaded.keys()))
print("Uploaded file:", image_path)

# Loading my trained model
model_path = "/content/fer_supervised_model.h5"
model = load_model(model_path)
print("Model loaded successfully.")

# Classes
class_labels = {
    0: 'Angry',
    1: 'Disgust',
    2: 'Fear',
    3: 'Happy',
    4: 'Sad',
    5: 'Surprise',
    6: 'Neutral'
}

# Face detection
def detect_and_predict_emotion(image_path, model):
    # Load Haar Cascade for face detection
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Reading the uploaded image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Error: Could not load {image_path}")
        return

    # Convert to grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    if len(faces) == 0:
        print("No face detected in the uploaded image.")
        show_image_with_label(img, "No Face Detected")
        return
    else:
        x, y, w, h = faces[0]
        face_roi = gray[y:y+h, x:x+w]
        face_resized = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_LINEAR)
        face_resized = face_resized.reshape(1, 48, 48, 1).astype('float32') / 255.0

        # Predicting the emotion
        preds = model.predict(face_resized)
        pred_class = np.argmax(preds[0])
        emotion_label = class_labels.get(pred_class, "Unknown")

        # Draw the box around the face
        cv2.rectangle(img, (x, y), (x+w, y+h), (0,255,0), 2)
        cv2.putText(img, emotion_label, (x, y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)

        # Displaying the final image
        show_image_with_label(img, f"Predicted Emotion: {emotion_label}")

def show_image_with_label(bgr_img, title):
    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(8, 8))
    plt.imshow(rgb_img)
    plt.title(title)
    plt.axis("off")
    plt.show()
detect_and_predict_emotion(image_path, model)

"""# ***Testing my model using webcam captured images***"""

# Capture a photo from the webcam
def take_photo(fname='captured_photo.jpg', quality=0.8):
    js = f"""
    async function takePhoto(quality) {{
      const div = document.createElement('div');
      const btn = document.createElement('button');
      btn.textContent = 'Capture';
      div.appendChild(btn);
      document.body.appendChild(div);
      const video = document.createElement('video');
      video.style.display = 'block';
      document.body.appendChild(video);
      const stream = await navigator.mediaDevices.getUserMedia({{video: true}});
      video.srcObject = stream;
      await new Promise(resolve => video.onloadedmetadata = resolve);
      video.play();
      await new Promise(resolve => btn.onclick = resolve);
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(track => track.stop());
      video.remove();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }}
    takePhoto({quality});
    """
    data = output.eval_js(js)
    binary = base64.b64decode(data.split(',')[1])
    img = cv2.imdecode(np.frombuffer(binary, np.uint8), cv2.IMREAD_COLOR)
    cv2.imwrite(fname, img)
    print("Photo saved as", fname)
    return fname

# Load your trained model and define class labels
model = load_model("fer_supervised_model.h5")
labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}
print("Model loaded successfully.")

# Capture image detect, face predict emotion, and display result
def capture_and_predict():
    path = take_photo()
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    img = cv2.imread(path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 5)
    if len(faces) == 0:
        print("No face detected.")
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.show()
        return
    x, y, w, h = faces[0]
    roi = gray[y:y+h, x:x+w]
    roi = cv2.resize(roi, (48,48))
    roi = roi.reshape(1,48,48,1).astype('float32')/255.0
    pred = model.predict(roi)
    emo = labels[np.argmax(pred)]
    cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0),2)
    cv2.putText(img, emo, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.title(f"Predicted Emotion: {emo}")
    plt.axis('off')
    plt.show()

capture_and_predict()